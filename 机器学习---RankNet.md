#RankNet
###介绍
RankNet是一种Pairwise方法。它不对单独的point去训练，而是针对pair去训练。因为是应用在排序上，我们更关心的是最终的前后关系，而非每个doc的具体得分。RankNet的最终目标就是训练出一个算分函数f，使得在所有pair上的排序估计损失最小。<br>

解决机器学习问题，需要将问题拆分成如下的子问题：问题的表述，目标model的形式，loss函数，优化方法。下文中将分别体现。<br>

###文档对&实例
那么如何通过pair来训练，并最终作用到针对point的算分函数上，请看下面的简单例子。假设有以下同一个query下的4个文档，并且有3个特征维度，并对它们进行了人工打分，我们就利用它们来训练model。
```
point       特征f1  特征f2  特征f3  label     
文档doc1      3       2        1      3(很好) 
文档doc2      1       2        1      2（好） 
文档doc3      1       1        2      1（一般） 
文档doc4      1       0        3      0（不好） 
```
可以认为有一个可以计算文档score的函数f，为了简单不妨先设其为一个线性函数，即<br>
```f(x)=w[1]*x[1]+ w[2]*x[2]+…+ w[n]*x[n]+b```        (1)

我们并不知道每个特征维度的具体权值w[i]。但我们感觉到要训练出这样的结果，就是使label得分比较高的样本得分尽量大，让label低的样本得分尽量小。

即本例中，我们应该让f(doc1)得分尽量大，f(doc4)得分尽量小。其实也就是f(doc1)-f(doc4)的结果尽量大，因为这个分差实际就包含了前者尽量大后者尽量小的含义。并且之前那个常量b通过作差后，不见了。

这样两两文档即可组成一个文档对，我们把前者好于后者的成为正向文档对；反之称之为负向文档对。正向对表示前者好于后者，负向对表示后者劣于前者。其实表述的是同样的意义。因此，我们只需要正向的对就好了。因为负向对不能再额外提供有意义的信息了。

那么我们提取出上面的正向对：<br>
```
Pair  作差       特征f1 特征f2  特征f3
P12 (doc1-doc2)    2      0       0
P13 (doc1-doc3)    2      1      -1
P14 (doc1-doc4)    2      2      -2
P23 (doc2-doc3)    0      1      -1
P24 (doc2-doc4)    0      2      -2
P34 (doc3-doc4)    0      1      -1

```

我们再引入一个文档对概率Pij，表示文档i好于文档j的概率。我们令
```Pij = exp{f(doc_i)-f(doc_j)} / (1+ exp{f(doc_i)-f(doc_j)})```           (2)<br>
这是sigmoid函数，其取值在(0,1)之间。
 
那么，现在这个问题就转化成了使所有正向对的概率和最大。<br>
   因为f(doc_i)-f(doc_j)其实也就是f(Dij)，所以有：
```Pij = exp{f(Dij)} / (1+ exp{ f(Dij)}) ```       (3)<br>
为了使Pij有一个参照标准，我们引入P’ij来表示根据真实已知的概率：i好于j时；P’ij=1，i差于j时，P’ij=0；i和j相等时，P’ij=0.5。<br>

现在我们回到公式(1)，由于我们不知道具体权重该取多少合适，那就先取一个随机值w1=0.1,w2=0.1,w3=0.1。此时，根据公式(1)(3)有：<br>
```
Pair  作差      特征f1  特征f2  特征f3  P     P’
D12 (doc1-doc2)   2       0       0        0.55   1
D13 (doc1-doc3)   2       1      -1        0.55   1
D14 (doc1-doc4)   2       2      -2        0.55   1
P23 (doc2-doc3)   0       1      -1        0.5    1
D24 (doc2-doc4)   0       2      -2        0.5    1
D34 (doc3-doc4)   0       1      -1        0.5    1
（因为都是取的正序对，所以P’均为1） 
```
现在我们有了对pair计算概率的方法了，也有了其对应的目标值了。该说说loss函数了。我们采用交叉熵作为其损失函数：<br>
```
Cij  = -P'ij*log(Pij)-(1-P'ij)log(1-Pij)    (4)
由于 Pij = exp(f(Dij))/(1+exp(f(Dij)))，代入上式有:
Cij = -P'ij*f(Dij)+log(1+exp(f(Dij)))        (5)
```
我们的目的就是使损失函数ΣCij最小，我们采用梯度下降的方法。<br>
```
Oij=f(Dij)=w[i]*x[i]，则Cij对w[i]求偏导数：
∂Cij/ ∂w[i] = -x[i] + x[i]*(exp{f(Dij)}/(1+ exp{f(Dij)}))          (6)
```
根据梯度下降更新参数的形式更新参数：<br>
```
w[i] = w[i] - η* ∂C/ ∂w[i]                   (7)
以D12一个点为例，令η=0.001代入：
w[i] = w[i]- η*x[i]*(1+2*exp(Oij))/ (1+exp(Oij))
```
具体如下：<br>
w[1] =w[1]- 0.001*x[1]*(1+2*exp(0.2))/ (1+exp(0.2)) = 0.1031<br>
w[2] =w[2]- 0 =0.1<br>
w[3] =w[3]- 0 =0.1<br>
通过代入一个点，使w[1]变大了一点点，而w[2],w[3]没有改变。<br>
我们把每个点一次代入，遍历一遍称为一轮。<br>
经过10轮之后，参数有之前的[0.1,0.1,0.1]变为：<br>
[0.19576186663713915, 0.2100450127644845, -0.010045012764484416]<br>

可见前两个参数是正向特征，后面一个是负向特征。此时再将训练好的参数代入之前的单点得分计算函数，即公式(1)中：<br>
```
      f1  f2  f3  label   初始计算得分值 10轮后计算值 
doc1  3   2    1   3(很好)     0.6            0.99 
doc2  1   2    1   2（好）     0.4            0.59 
doc3  1   1    2   1（一般）   0.4            0.38 
doc4  1   0    3   0（不好）   0.4            0.17
```
可见，在最初时得分函数并不能很好的估量文档的好坏，而经过几轮迭代优化之后得分值和预期label可以很好的匹配。

综上，就是在pair的基础上，通过梯度下降的方法来优化交叉熵，从而获得一个得分函数。


###梯度下降代码
```python
import sys
import math  
 
x = [(2,0,0),(2,1,-1),(2,2,-2),(0,1,-1),(0,2,-2),(0,1,-1)]  

alpha = 0.001 # 步长  
m = len(x) # 训练数据条数  
w = [0.1,0.1,0.1]  #初始随机设置权值
loop_max = 10 # 最大迭代次数  
count = 1  

while count<=loop_max:
    print ('第%d轮：'%count) 
    count += 1   
    # 遍历训练数据集，不断更新w值
    for i in range(m): 
        sum = w[0]*x[i][0]+w[1]*x[i][1]+w[2]*x[i][2]
        w[0] = w[0]+alpha*x[i][0]*(1+2*math.exp(sum))/(1+math.exp(sum))
        w[1] = w[1]+alpha*x[i][1]*(1+2*math.exp(sum))/(1+math.exp(sum))
        w[2] = w[2]+alpha*x[i][2]*(1+2*math.exp(sum))/(1+math.exp(sum))
    print w
    
```

参考：<br>
1《Learning to Rank using Gradient Descent》<br>
2 《Learning to Rank for Information Retrieval》<br>

--------------------------------
######（转载本站文章请注明作者和出处 <a href="https://github.com/MangoLiu">MangoLiu</a> ，请勿用于任何商业用途）
