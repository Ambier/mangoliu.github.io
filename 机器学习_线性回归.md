#机器学习--回归
##介绍
线性回归属于监督学习的一种，用于模型为连续函数的数值预测。也可以利用回归方程来做二分类。<br>

线性回归其实就是确定一系列最佳参数。显而易见，参数的最后确定和训练样本有关，和假设参数有关，也和损失函数有关。<br>

##简单约定
假设是线性关系：<br>
**h(x)=θ[0]x[0] + θ[1]x[1] +...**<br>
其中θ[i]是我们要估计的参数，x[i]是自变量，即特征；y[i]是因变量，即结果。<br>

我们采用**平方误差和**的形式来作为损失函数:<br>
**J(θ)= 1/2 * {[h(x[0])-y[0]]^2 + [h(x[1])-y[1]]^2 + ...}**<br>
说明:1/2只是为了后面求导计算时方便而特地加上的。<br>

此时，我们的目的就很明确了：利用已有的训练数据，来得到一些系列θ，使J(θ)的值达到最小。<br>

##梯度法
梯度上升法基本思想：要找到某个函数的最大值，最好的方法是沿着该函数的梯度方向探寻。因为这是函数值增长最快的方向。<br>

梯度下降法和上升法的思想是一致的，只是使用的不是加法而是减法。上升法是求最大值，下降法是求最小值。<br>
在此，我们要求最小值，所以我们使用梯度下降法。<br>
![公式1](/images/jiqixuexi/ML_LR_1.png)<br>
![公式2](/images/jiqixuexi/ML_LR_2.png)<br>
这样，梯度下降的公式就变成了这样：<br>
![公式3](/images/jiqixuexi/ML_LR_3.png)<br>

这里的α为**学习速率**，即每次迭代的步长，如果太小则迭代速度过慢，如果太大则因为跨度太大无法有效找到期望的最小值，该值需要根据实际情况进行调整。<br>

我们一次把所有的数据都考虑进来，称作是“**批处理**”：<br>
![公式4](/images/jiqixuexi/ML_LR_4.png)<br>
这样做计算复杂度太高。我们可以使用一种新的方法，一次仅用一个样本来更新回归系数，称之为“**随机梯度上升算法**”。这是一种在线学习算法，进行增量式更新。<br>

请看下例：<br>
```python
# -*- coding:utf-8 -*-  
""" 
增量梯度下降 
y=1+0.5x 
"""  
import sys  
  
  
# 训练数据集  
# 自变量x(x0,x1)  
x = [(1,1.15),(1,1.9),(1,3.06),(1,4.66),(1,6.84),(1,7.95)]  
# 假设函数 h(x) = theta0*x[0] + theta1*x[1]  
# y为理想theta值下的真实函数值  
y = [1.37,2.4,3.02,3.06,4.22,5.42]  
  
  
# 两种终止条件  
loop_max = 10000 # 最大迭代次数  
epsilon = 0.0001 # 收敛精度  
  
  
alpha = 0.005 # 步长  
diff = 0 # 每一次试验时当前值与理想值的差距  
error0 = 0 # 上一次目标函数值之和  
error1 = 0 # 当前次目标函数值之和  
m = len(x) # 训练数据条数  
  
  
#init the parameters to zero  
theta = [0,0]  
  
count = 0  
finish = 0  
while count<loop_max:  
    count += 1  
    # 遍历训练数据集，不断更新theta值  
    for i in range(m):  
        # 训练集代入，计算假设函数值h(x)与真实值y的误差值  
        diff = (theta[0]*x[i][0] + theta[1]*x[i][1]) - y[i]  
      
        # 求参数theta，增量梯度下降算法，每次只使用一组训练数据  
        theta[0] = theta[0] - alpha * diff * x[i][0]  
        theta[1] = theta[1] - alpha * diff * x[i][1]  
    # 此时已经遍历了一遍训练集，求出了此时的theta值  
    
    # 判断是否已收敛  
    if abs(theta[0]-error0) < epsilon and abs(theta[1]-error1) < epsilon:  
        print 'h(x) = %f * x0 + %f * x1 '%(theta[0],theta[1]) 
        finish = 1  
    else:  
        error0,error1 = theta  
    if finish:  
        break  
  
print 'FINISH count:%s' % count

```




可以考虑α随着迭代次数的增加，α逐渐减小，但仍要具有影响。<br>
为了避免参数的严格下降也可考虑使用模拟退火等算法。<br>




--------------------------------
最简单的线性回归<br>
1 如何选择初始点,即初始参数值的确定？<br>
2 如何防止过拟合？<br>
3 如何选择合适的参数alpha？<br>
4 最小二乘法和梯度下降法的区别是什么？<br>
5 如何设计终止条件？<br>
6 在上面的过程中有哪些可以优化的地方？<br>
7 岭回归<br>
8 牛顿法和拟牛顿法<br>
9 在4和8提及方法有何本质区别？<br>


--------------------------------
######（转载本站文章请注明作者和出处 <a href="https://github.com/MangoLiu">MangoLiu</a> ，请勿用于任何商业用途）

