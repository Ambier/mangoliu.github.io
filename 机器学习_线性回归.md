#机器学习--回归
##介绍
线性回归属于监督学习的一种，用于模型为连续函数的数值预测。也可以利用回归方程来做二分类。<br>

线性回归其实就是确定一系列最佳参数。显而易见，参数的最后确定和训练样本有关，和假设参数有关，也和损失函数有关。<br>

##简单约定
假设是线性关系：<br>
**h(x)=θ[0]x[0] + θ[1]x[1] +...**<br>
其中θ[i]是我们要估计的参数，x[i]是自变量，即特征；y[i]是因变量，即结果。<br>

我们采用**平方误差和**的形式来作为损失函数:<br>
**J(θ)= 1/2 * {[h(x[0])-y[0]]^2 + [h(x[1])-y[1]]^2 + ...}**<br>
说明:1/2只是为了后面求导计算时方便而特地加上的。<br>

此时，我们的目的就很明确了：利用已有的训练数据，来得到一些系列θ，使J(θ)的值达到最小。<br>

##梯度法
梯度上升法基本思想：要找到某个函数的最大值，最好的方法是沿着该函数的梯度方向探寻。因为这是函数值增长最快的方向。<br>

梯度下降法和上升法的思想是一致的，只是使用的不是加法而是减法。上升法是求最大值，下降法是求最小值。<br>
在此，我们要求最小值，所以我们使用梯度下降法。<br>
![公式1](/images/jiqixuexi/ML_LR_1.png)<br>
![公式2](/images/jiqixuexi/ML_LR_2.png)<br>
这样，梯度下降的公式就变成了这样：<br>
![公式3](/images/jiqixuexi/ML_LR_3.png)<br>

这里的α为**学习速率**，即每次迭代的步长，如果太小则迭代速度过慢，如果太大则因为跨度太大无法有效找到期望的最小值，该值需要根据实际情况进行调整。<br>

我们一次把所有的数据都考虑进来，称作是“**批处理**”：<br>
![公式4](/images/jiqixuexi/ML_LR_4.png)<br>
这样做计算复杂度太高。我们可以使用一种新的方法，一次仅用一个样本来更新回归系数，称之为“**随机梯度上升算法**”。这是一种在线学习算法，进行增量式更新。<br>





可以考虑α随着迭代次数的增加，α逐渐减小，但仍要具有影响。<br>
为了避免参数的严格下降也可考虑使用模拟退火等算法。<br>




--------------------------------
最简单的线性回归
1 如何选择初始点,即初始参数值的确定？<br>
2 如何防止过拟合？<br>
3 如何选择合适的参数alpha？<br>
4 最小二乘法和梯度下降法的区别是什么？<br>
5 如何设计终止条件？<br>
6 在上面的过程中有哪些可以优化的地方？<br>
7 岭回归<br>
8 牛顿法和拟牛顿法<br>
9 在4和8提及方法有何本质区别？<br>


--------------------------------
######（转载本站文章请注明作者和出处 <a href="https://github.com/MangoLiu">MangoLiu</a> ，请勿用于任何商业用途）

