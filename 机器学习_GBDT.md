##机器学习--AdaBoost和GBDT
###决策树、模型组合
--------------------------------
**决策树**这种算法有着很多良好的特性，比如说**训练时间复杂度较低，预测的过程比较快速，模型容易展示**（容易将得到的决策树做成图片展示出来）等。但是同时，单决策树又有一些不好的地方，比如说**容易over-fitting**，虽然有一些方法，如剪枝可以减少这种情况，但是还是不够的。

**模型组合（比如说有Boosting，Bagging等）**与决策树相关的算法比较多，这些算法最终的结果是生成N(可能会有几百棵以上）棵树，这样可以大大的减少单决策树带来的毛病，虽然这几百棵决策树中的每一棵都很简单（相对于C4.5这种单决策树来说），但是他们组合起来确是很强大。

模型组合+决策树相关的算法有两种比较基本的形式 – **随机森林与GBDT**((Gradient Boost Decision Tree)，其他的比较新的模型组合+决策树的算法都是来自这两种算法的延伸。

###Adaboost
--------------------------
**Boosting**的基本思想很简单，就是"三个臭皮匠顶个诸葛亮"。将若干个弱分类器(base learner)组合起来，变成一个强分类器。大多数boosting方法都是通过不断改变训练数据的概率(权值)分布，来迭代训练弱学习器的。所以总结而言，boosting需要回答2个问题:

**如何改变训练数据的概率(权值)分布**
**如何将弱分类器组合起来**
下面先用Adaboost入手，聊一下boosting。

**输入**: 训练样例T，代表原始训练集。
```
T={(x1,y1),(x2,y2),...,(xn,yn)}
```
      
**输出**: 由M个弱分类器构成的最终分类器G(x)。<br>
我们要训练出来M个弱分类器，然后把它们组合成为一个最终分类器G(x)

**具体步骤**：<br>
1.**初始化权值分布**:<br>
```
D1 = (w11,...,w1i,...,w1n),w1i=1/n
```
D是对应n个训练样本权重的集合。后面的索引代表的是第几轮训练。这里初始的第一轮，把每个样本设定为权重相等，为1/n。（也可以设定为别的值。）

2 **使用带权值的实例集合Dm训练模型，得到弱分类器**:<br>
```
Gm(x):x−>y
```
这里的G是一个弱分类器，后面的m是代表第几轮的训练。也可以把它看作是使用Dm训练集的分类器。<br>
这里需要说明的是；训练集D是根据每次训练以后不断修改其里面每个元素的权重的。

3.**计算Gm(x)在训练集上的误差率**：<br>
![ml-gbdt-1.png](/images/jiqixuexi/ml-gbdt-1.png)<br>
这里的误差率其实就是分错的样本*对应权重，再求和。<br>

4.**计算Gm(x)的系数**:<br>
![ml-gbdt-2.png](/images/jiqixuexi/ml-gbdt-2.png)<br>
因为我们最后是要将整个过程中得到的一系列弱分类器关联起来，那么这个分类器在整个体系中的系数是多少呢？就是这个a。可以看出当错误率接近0，则a是一个很大的正值；而当错误率接近1时，a是一个很大的负值。<br>
即这个分类器的整体误差决定了它在最终分类器中的权重。

5.**更新训练样本的权值分布，为下一轮迭代做准备**：<br>
因为adaboost就是不断的更改样本的权重，是上一轮错分的数据权重变大，使分类正确的数据权重变小。<br>
下一轮数据集为Dm+1，其中每个数据权重更新如下：<br>
![ml-gbdt-3.png](/images/jiqixuexi/ml-gbdt-3.png)<br>
若是有些不明白上面式子的含义，可以这样理解：<br>
![ml-gbdt-4.png](/images/jiqixuexi/ml-gbdt-4.png)<br>
可以看出权值更新的幅度与其整体误差（也就是a）是紧密相关的。而规范化因子的作用是使sum(Wi)等于1<br>

6 **进行了M轮迭代之后，产出了M个弱分类器，将它们组合起来**：<br>
![ml-gbdt-5.png](/images/jiqixuexi/ml-gbdt-5.png)<br>

##GBDT
--------------------------
GBDT是一个应用很广泛的算法，可以用来做分类、回归。在很多的数据上都有不错的效果。
回归是用拟合残差，分类是用错误率来调整样本权值。
   
GBDT这个算法还有一些其他的名字，比如说MART(Multiple Additive Regression Tree)，GBRT(Gradient Boost Regression Tree)，TreeNet等，其实它们都是一个东西(参考自wikipedia – Gradient Boosting)

Gradient Boost其实是一个框架，里面可以套入很多不同的算法。

Boost是"提升"的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。

原始的Boost算法是在算法开始的时候，为每一个样本赋上一个权重值，初始的时候，大家都是一样重要的。

在每一步训练中得到的模型，会使得数据点的估计有对有错，我们就在每一步结束后，增加分错的点的权重，减少分对的点的权重，这样使得某些点如果老是被分错，那么就会被“严重关注”，也就被赋上一个很高的权重。

然后等进行了N次迭代(由用户指定)，将会得到N个简单的分类器(basic learner)，我们将它们组合起来(比如说可以对它们进行加权、或者让它们进行投票等)，得到一个最终的模型。

而Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。

所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。

在分类问题中，有一个很重要的内容叫做Multi-Class Logistic，也就是多分类的Logistic问题，它适用于那些类别数>2的问题，并且在分类结果中，样本x不是一定只属于某一个类，可以得到样本x分别属于多个类的概率(也可以说样本x的估计y符合某一个几何分布)，这实际上是属于Generalized Linear Model中讨论的内容。

这里就用一个结论：如果一个分类问题符合几何分布，那么就可以用Logistic变换来进行之后的运算。

假设对于一个样本x，它可能属于K个分类，其估计值分别为F1(x)…FK(x)，Logistic变换如下，logistic变换是一个平滑且将数据规范化(使得向量的长度为1)的过程，结果为属于类别k的概率pk(x)，

 image

   对于Logistic变换后的结果，损失函数为：

 image   

  其中，yk为输入的样本数据的估计值，当一个样本x属于类别k时，yk = 1，否则yk = 0。
将Logistic变换的式子带入损失函数，并且对其求导，可以得到损失函数的梯度：

 image   

上面说的比较抽象，下面举个例子：
假设输入数据x可能属于5个分类(分别为1,2,3,4,5)，训练数据中，x属于类别3，则y = (0, 0, 1, 0, 0)，假设模型估计得到的F(x) = (0, 0.3, 0.6, 0, 0)，则经过Logistic变换后的数据p(x) = (0.16,0.21,0.29,0.16,0.16)，y - p得到梯度g：(-0.16, -0.21, 0.71, -0.16, -0.16)。
观察这里可以得到一个比较有意思的结论：

假设gk为样本当某一维(某一个分类)上的梯度:
gk>0时，越大表示其在这一维上的概率p(x)越应该提高，比如说上面的第三维的概率为0.29，就应该提高，属于应该往“正确的方向”前进越小表示这个估计越“准确”
    
gk<0时，越小，负得越多表示在这一维上的概率应该降低，比如说第二维0.21就应该得到降低。属于应该朝着“错误的反方向”前进越大，负得越少表示这个估计越“不错误 ”
    
总的来说，对于一个样本，最理想的梯度是越接近0的梯度。所以，我们要能够让函数的估计值能够使得梯度往反方向移动(>0的维度上，往负方向移动，<0的维度上，往正方向移动)最终使得梯度尽量=0)，并且该算法在会严重关注那些梯度比较大的样本，跟Boost的意思类似。
    
得到梯度之后，就是如何让梯度减少了。这里是用的一个迭代+决策树的方法，当初始化的时候，随便给出一个估计函数F(x)(可以让F(x)是一个随机的值，也可以让F(x)=0)，然后之后每迭代一步就根据当前每一个样本的梯度的情况，建立一棵决策树。就让函数往梯度的反方向前进，最终使得迭代N步后，梯度越小。

这里建立的决策树和普通的决策树不太一样，首先，这个决策树是一个叶子节点数J固定的，当生成了J个节点后，就不再生成新的节点了。

算法的流程如下:(参考自treeBoost论文)
0. 给定一个初始值
1. 建立M棵决策树(迭代M次)
2. 对函数估计值F(x)进行Logistic变换
3. 对于K个分类进行下面的操作(其实这个for循环也可以理解为向量的操作，每一个样本点xi都对应了K种可能的分类yi，所以yi, F(xi), p(xi)都是一个K维的向量，这样或许容易理解一点)
4. 求得残差减少的梯度方向
5. 根据每一个样本点x，与其残差减少的梯度方向，得到一棵由J个叶子节点组成的决策树
6. 为当决策树建立完成后，通过这个公式，可以得到每一个叶子节点的增益(这个增益在预测的时候用的)

每个增益的组成其实也是一个K维的向量，表示如果在决策树预测的过程中，如果某一个样本点掉入了这个叶子节点，则其对应的K个分类的值是多少。比如 说，GBDT得到了三棵决策树，一个样本点在预测的时候，也会掉入3个叶子节点上，其增益分别为(假设为3分类的问题)：(0.5, 0.8, 0.1),  (0.2, 0.6, 0.3),  (0.4, 0.3, 0.3)
那么这样最终得到的分类为第二个，因为选择分类2的决策树是最多的。
     
7. 的意思为，将当前得到的决策树与之前的那些决策树合并起来，作为新的一个模型(跟6中所举的例子差不多)
     
实现：
     看明白了算法，就需要去实现一下，或者看看别人实现的代码，这里推荐一下wikipedia中的gradient boosting页面，下面就有一些开源软件中的一些实现，比如说下面这个：http://elf-project.sourceforge.net/ 

-----
wikipedia – Gradient Boosting)
treeBoost论文
[机器学习笔记3 - Boosting方法](http://blog.crackcell.com/posts/2013/04/30/machine_learning_note_3_boosting.html)

--------------------------------
######(转载本站文章请注明作者和出处 <a href="https://github.com/MangoLiu">MangoLiu</a> ，请勿用于任何商业用途)