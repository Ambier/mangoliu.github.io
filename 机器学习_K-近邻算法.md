#机器学习_K-近邻算法
##介绍
--------------------------------
举个例子，有个简单的认识：<br>
>我们如何判断一部影片是爱情片还是武打片？（假设：我们已经统计了其中的接吻镜头次数和打斗镜头次数）A影片有11次打斗，2次接吻镜头；而B影片有4次打斗，32次接吻镜头（咦？什么电影接吻次数这么多？哈！这是假设，千万别当真，继续往下看）；那么，我们会认为A是武打片，B是爱情片。我们为什么会这样认为？因为A更接近于武打片，而B更接近于爱情片。<br>

其实，K-近邻算法的思想和上面的差不多。简单地说，K-近邻算法采用测量不同特征值之间的距离方法进行分类。<br>
决策树是根据训练数据来构造的，之后可用于分析数据，或用作预测。<br>
优点：直观，精度高，对异常值不敏感。<br>
缺点：空间复杂度高，计算量大。<br>
原理：<br>
> 存在一个样本数据集合，样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征样本集中数据对应特征进行比较，然后提取样本集中特征最相似(最近邻)的分类标签。一般来说，我们只选择样本数据集前K个最相似的数据。最后，选择K个最相似数据中出现次数最后的分类，作为新数据的分类。<br>

有兴趣可以先看看关于最近邻居算法的介绍：[最近邻居算法(wikipedia)](http://zh.wikipedia.org/wiki/K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95

## 决策树
--------------------------------
在学习如何选取最好的特征来划分数据之前，我们先来学习下<strong>熵</strong>(entropy)的概念。熵是集合信息的一种度量方式，可以表示划分数据集的前后信息发生的变化。它的公式为：<br>
![熵](/images/jiqixuexi/jueceshu_shang.png)
<br>
假设分类后，有C种结果，每种的概率为p(x)，结果切分的越均匀，entropy的值越大。所以，我们的目标是计算每一种特征的熵，选择最大值的特征，作为当前分类的判断节点。<br>
关于熵的介绍：[熵(wikipedia)](http://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5)
<br>
另一种度量集合无序程度的方法是基尼不纯度，简单地说就是从一个数据集中随机选取子项，度量其被错误分类到其他分组的概率。基尼不纯度可以通过如下计算：<br>
![基尼不纯度](/images/jiqixuexi/jueceshu_jini.PNG)<br>
<br>
然后。对每个特征划分一次数据集，计算数据集的新熵值，之后比较并返回最好特征划分的索引值。<br>

## 递归构建决策树
--------------------------------
划分一次后，数据将向下传递到树分支的下个节点，在这个节点上，我们可以再次划分数据，因此我们可以采用递归的原则处理数据集。但递归总是要有结束条件的，这里的结束条件是什么呢？<br>

递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。若得到的关键字的值是类标签，则表示其是一个叶子节点，否则是一个判断节点，继续构造。<br>

但是假如数据集已经处理了所有的属性，但是类标签依然不是唯一的。怎么办？我们通常会采用多数表决非方法决定该叶子节点的分类。<br>
构造决策树的目的在于后续的使用，若是每次都根据训练集来生成，则会消耗很多时间。因此最好将生成好的决策树进行序列化操作，保存在硬盘上，并在需要的时候读取出来。<br>

## 总结
--------------------------------
假如给定一些包含各种特征的数据，我们应该这样做：<br>

>1.假如当前的数据都属于同一类，则结束计算划分，并添加终止模块，其中记录当前的类标识；<br>
 2.若是所有的属性都已消耗完，剩余的数据不属于同一类别，则按照多数表决的方式决定；<br>
 3.遍历每个特征来划分数据（每划分一次，消耗一个特征），对每个结果集求熵，选取最大的作为当前划分特征；（若是熵有相同的情形，任选一个）<br>
 4.否则，按照上述递归构建子树；<br>
 5.序列化保存构建好的决策树，方便后续使用。


--------------------------------
######（转载本站文章请注明作者和出处 https://github.com/MangoLiu ，请勿用于任何商业用途）
