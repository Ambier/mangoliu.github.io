#机器学习_K-近邻算法
##介绍
--------------------------------
举个例子，有个简单的认识：<br>
>我们如何判断一部影片是爱情片还是武打片？（假设：我们已经统计了其中的接吻镜头次数和打斗镜头次数）A影片有11次打斗，2次接吻镜头；而B影片有4次打斗，32次接吻镜头（咦？什么电影接吻次数这么多？哈！这是假设，千万别当真，继续往下看）；那么，我们会认为A是武打片，B是爱情片。我们为什么会这样认为？因为A更接近于武打片，而B更接近于爱情片。
<br>

其实，K-近邻算法的思想和上面的差不多。简单地说，K-近邻算法采用测量不同特征值之间的距离方法进行分类。<br>
决策树是根据训练数据来构造的，之后可用于分析数据，或用作预测。<br>
优点：直观，精度高，对异常值不敏感。不仅可以解决二分类，还可以解决多分类问题。<br>
缺点：空间复杂度高，计算量大。<br>
原理：<br>
>存在一个样本数据集合，样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征样本集中数据对应特征进行比较，然后提取样本集中特征最相似(最近邻)的分类标签。一般来说，我们只选择样本数据集前K个最相似的数据。最后，选择K个最相似数据中出现次数最后的分类，作为新数据的分类。
<br>

实际应用时需要考虑：<br>
1 如何度量样本之间的距离。<br>
2 如何确定合适的K值。<br>
3 选择出前k个值后，如何判定类别。<br>

此外，KNN需要对各个维度进行归一化。

有兴趣可以先看看关于最近邻居算法的介绍：[最近邻居算法(wikipedia)](http://zh.wikipedia.org/wiki/K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95)

--------------------------------
######（转载本站文章请注明作者和出处 https://github.com/MangoLiu ，请勿用于任何商业用途）
