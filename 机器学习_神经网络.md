#机器学习--神经网络

感知器就是一个最简单的单层前馈神经网络。<br>
几个输入节点，一个输入节点。每个输入节点进行加权后求和，若是高于某阈值，则输出1，否则输入0。<br>
![感知器](/images/jiqixuexi/ML_ANN1.png)<br>

训练一个感知器模型就是不断调整链的权值，知道能满足关系为止。<br>
具体算法如下:<br>
![感知器算法](/images/jiqixuexi/ML_ANN2.png)<br>
通过w=w+λ(目标值-预测值)来补偿误差。<br>
这里的λ成为学习率。过大会比较敏感，太小会导致收敛过慢。可以使用一个自适应的λ值。<br>

对于线性可分的分类问题，感知器学习算法保证收敛到最优解（只要学习率足够小，网上有关于它收敛性的证明）<br>
```python
#!/usr/bin/env python
#encoding=utf-8
#线性可分情况下的简单感知器模型
a = 0.0001
w = [0.3,0.2,0.2]
data = [[1,0,0,-1],[1,0,1,1],[1,1,0,1],[1,1,1,1],[0,0,1,-1],[0,1,0,-1],[0,1,1,1],[0,0,0,-1]]
YUZHI = 0.4

while True:
    flag = True
    for line in data:
        T = w[0]*line[0]+w[1]*line[1]+w[2]*line[2] - YUZHI
        T = 1 if T>0 else -1

        if line[3]!=T:
            flag = False
       
        for i in range(len(w)):
            w[i] = w[i] + a*(line[3]-T)

    if flag:
        break

print 'The result is :',
for wi in w:
    print wi,     
```
问题:这里的阈值t值怎样设定和调节的呢？<br>
答：可以设定将t也作为一个参数，只是它的输入恒定为1。<br>

神经网络需要很长的训练时间。<br>
需要大量的参数，通常主要靠经验确定，如网络拓扑结构。<br>
神经网络常常因为可解释性差而受到批评。例如，人们很难解释网络中学习的权重和“隐含单元”的符号意义。<br>
然而，神经网络的优点包括其对噪声数据的高承受能力，以及对未经训练的数据模式分类能力。<br>
在缺乏属性和类之间的联系的知识时可以使用它们。<br>
不像大部分决策树算法，它们非常适合连续值的输入和输出。<br>
神经网络算法是固有并行的，可以使用并行技术来加快计算过程。<br>

每个输出单元取前一层单元输出的加权和作为输入。它将一个非线性（激励）函数作用于加权输入。多层前馈神经网络能够将类预测作为输入的非线性组合建模。从统计学的观点来讲，它们进行非线性回归。给定足够多的隐藏单元和足够的训练样本，多层前馈神经网络可以逼近任何函数。<br>

确定网络拓扑：在开始训练之前，用户必须说明输入层的单元数、隐藏层数（如果多于一层）、每个隐藏层的单元数和输出层的单元数，以确定网络拓扑。<br>

归一化：对训练元组中每个属性的测量输入值进行归一化将有助于加快学习过程。通常，对输入值进行归一化，使其落入0.0~1.0之间。离散值属性可以重新编码，使离散属性的每个阈值对应一个输入单元。例如，如果属性A有三个可能的取值{a0, a1, a2}，则可以分配三个输入单元表示A，即可以使用I0, I1, I2作为输入单元。每个单元初始化为0。如果A=a0，则I0置为1；如果A=a1，则I1置为1。<br>

输出：神经网络可以用来分类（预测给定元组的类标号）或预测（预测连续值输出）。对于分类，一个输出单元可以用来表示两个类（其中值1代表一个类，值0代表另一个类）。如果多于两个类，则每个类使用一个输出单元。<br>

对于“最好的”隐藏单元数，没有明确的规则。网络设计是一个试凑的过程，可以使用爬山法，从一个选择修改的初始结构开始。<br>

后向传播迭代地处理训练元组数据集，将每个元组的网络预测与实际已知的目标值进行对比，目标值可以是已知训练元组的类标号（对于分类问题）或连续值（对于预测问题）。<br>

对于每个训练样本，“由后向前”修改权重使网络预测和实际目标值之间的均方误差最小，即由输出层，经由每个隐藏层，到第一个隐藏层（因此称作后向传播）。尽管不能保证，一般地，权重将最终收敛，学习过程停止。<br>

初始化权重：网络的权重初始化为很小的随机数（例如，由-1.0~1.0，或由-0.5~0.5）。每个单元有一个与之关联的偏倚（Bias），偏倚也类似的初始化为小随机数。偏倚充当阈值，用来改变单元的活性。<br>

向前传播输入：首先，训练元组提供给网络的输入层。输入通过输入单元，不发生变化，也就是说，对于输入单元j，它的输出Oj等于它的输入值Ij。然后计算隐藏层和输出层每个单元的净输入和输出。隐藏层或输出层的净输入用其输入的线性组合计算。<br>

一个隐藏或输出单元j：单元j的输入是来自于上一层的输出。这些与对应的权重相乘，形成加权和。加权和再加到与单元j相关联的偏倚上。隐藏层和输出层的每个单元取其净输入，然后将激励函数作用于它，该函数用符号表现单元代表的神经元活性。通常使用Logistic或S形（Sigmoid）函数。该函数又称挤压函数（Squashing Function），因为它将一个较大的输入值映射到较小的区间0~1。Logistic函数是非线性的和可微的，使得后向传播算法可以对非线性可分的分类问题建模。<br>

实践中，由于在向后传播误差时还需要这些中间输出值，因此将每个单元的中间输出值存放起来是一个好办法，这种技巧可以显著的降低所需的计算量。<br>

反向传播误差：当实际输出与期望输出不符时，进入误差的反向传播阶段。误差通过输出层，按误差梯度下降的方式修正各层权值，向隐层、输入层逐层反传。周而复始的信息正向传播和误差反向传播过程，是各层权值不断调整的过程，也是神经网络学习训练的过程，此过程一直进行到网络输出的误差减少到可以接受的程度，或者预先设定的学习次数为止。<br>

综上：BP算法的主要步骤：<br>
1） 网络状态初始化；用较小的随机数对网络的权以及偏置置初值。<br>
2） 如样本数为P，则下面过程重复P次。<br>
    a. 输入学习样本；<br>
    b. 计算隐层及输出层输出；<br>
    c. 计算教师信号与实际值误差；<br>
    d. 累计c误差。<br>
3)误差精度或循环次数达到要求则到5），否则到4）。<br>
4）采用提高梯度法对任一神经元之间连接权及偏置进行调整，转回2）。<br>
5）输出结果。<br>

以下是Python的实现代码(可以通过以下代码来学习，写的很好)：<br>
```python
# Back-Propagation Neural Networks

import math
import random
import string

random.seed(0)

# calculate a random number where:  a <= rand < b
def rand(a, b):
    return (b-a)*random.random() + a

# Make a matrix (we could use NumPy to speed this up)
def makeMatrix(I, J, fill=0.0):
    m = []
    for i in range(I):
        m.append([fill]*J)
    return m

# our sigmoid function, tanh is a little nicer than the standard 1/(1+e^-x)
def sigmoid(x):
    return math.tanh(x)

# derivative of our sigmoid function, in terms of the output (i.e. y)
def dsigmoid(y):
    return 1.0 - y**2

class NN:
    def __init__(self, ni, nh, no):
        # number of input, hidden, and output nodes
        self.ni = ni + 1 # +1 for bias node
        self.nh = nh
        self.no = no

        # activations for nodes
        self.ai = [1.0]*self.ni
        self.ah = [1.0]*self.nh
        self.ao = [1.0]*self.no
        
        # create weights
        self.wi = makeMatrix(self.ni, self.nh)
        self.wo = makeMatrix(self.nh, self.no)
        # set them to random vaules
        for i in range(self.ni):
            for j in range(self.nh):
                self.wi[i][j] = rand(-0.2, 0.2)
        for j in range(self.nh):
            for k in range(self.no):
                self.wo[j][k] = rand(-2.0, 2.0)

        # last change in weights for momentum   
        self.ci = makeMatrix(self.ni, self.nh)
        self.co = makeMatrix(self.nh, self.no)

    def update(self, inputs):
        if len(inputs) != self.ni-1:
            raise ValueError('wrong number of inputs')

        # input activations
        for i in range(self.ni-1):
            #self.ai[i] = sigmoid(inputs[i])
            self.ai[i] = inputs[i]

        # hidden activations
        for j in range(self.nh):
            sum = 0.0
            for i in range(self.ni):
                sum = sum + self.ai[i] * self.wi[i][j]
            self.ah[j] = sigmoid(sum)

        # output activations
        for k in range(self.no):
            sum = 0.0
            for j in range(self.nh):
                sum = sum + self.ah[j] * self.wo[j][k]
            self.ao[k] = sigmoid(sum)

        return self.ao[:]


    def backPropagate(self, targets, N, M):
        if len(targets) != self.no:
            raise ValueError('wrong number of target values')

        # calculate error terms for output
        output_deltas = [0.0] * self.no
        for k in range(self.no):
            error = targets[k]-self.ao[k]
            output_deltas[k] = dsigmoid(self.ao[k]) * error

        # calculate error terms for hidden
        hidden_deltas = [0.0] * self.nh
        for j in range(self.nh):
            error = 0.0
            for k in range(self.no):
                error = error + output_deltas[k]*self.wo[j][k]
            hidden_deltas[j] = dsigmoid(self.ah[j]) * error

        # update output weights
        for j in range(self.nh):
            for k in range(self.no):
                change = output_deltas[k]*self.ah[j]
                self.wo[j][k] = self.wo[j][k] + N*change + M*self.co[j][k]
                self.co[j][k] = change
                #print N*change, M*self.co[j][k]

        # update input weights
        for i in range(self.ni):
            for j in range(self.nh):
                change = hidden_deltas[j]*self.ai[i]
                self.wi[i][j] = self.wi[i][j] + N*change + M*self.ci[i][j]
                self.ci[i][j] = change

        # calculate error
        error = 0.0
        for k in range(len(targets)):
            error = error + 0.5*(targets[k]-self.ao[k])**2
        return error


    def test(self, patterns):
        for p in patterns:
            print(p[0], '->', self.update(p[0]))

    def weights(self):
        print('Input weights:')
        for i in range(self.ni):
            print(self.wi[i])
        print()
        print('Output weights:')
        for j in range(self.nh):
            print(self.wo[j])

    def train(self, patterns, iterations=1000, N=0.5, M=0.1):
        # N: learning rate
        # M: momentum factor
        for i in range(iterations):
            error = 0.0
            for p in patterns:
                inputs = p[0]
                targets = p[1]
                self.update(inputs)
                error = error + self.backPropagate(targets, N, M)
            if i % 100 == 0:
                print('error %-.5f' % error)


def demo():
    # Teach network XOR function
    pat = [
        [[0,0], [0]],
        [[0,1], [1]],
        [[1,0], [1]],
        [[1,1], [0]]
    ]
    # create a network with two input, two hidden, and one output nodes
    n = NN(2, 2, 1)
    n.train(pat)
    n.test(pat)

if __name__ == '__main__':
    demo()
```





参考：<br>
1《数据挖掘导论》(有个基本的认识，特别是单层神经网络)<br>
2 [人工神经网络简论](http://www.cnblogs.com/chaosimple/archive/2013/06/11/3131852.html)<br>
3 [BP神经网络理论](http://www.cnblogs.com/hellope/archive/2012/07/05/2577814.html)<br>
4 [wiki--Backpropagation](http://en.wikipedia.org/wiki/Backpropagation)（不仅讲了BP的具体理论，还有各种语言实现的链接）<br>
5 [神经网络介绍--利用反向传播算法的模式学习](http://www.ibm.com/developerworks/cn/linux/other/l-neural/index.html)(讲的比较清晰，特别是delta规则和具体示例)<br>
6 [反向传导算法](http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)(Ng 主讲的课件)<br>

--------------------------------
######（转载本站文章请注明作者和出处 <a href="https://github.com/MangoLiu">MangoLiu</a> ，请勿用于任何商业用途）

