#机器学习_决策树
##介绍
--------------------------------
>举个简单的例子，先有个感性的认识：他人给介绍相亲时，总会问几个简单的问题，然后判断是否有见面的必要。这个过程实际上就是一个决策树。我们使用决策树来处理分类问题，它包含判断模块，终止模块。先看一个简单的决策树表示图：

![决策树举例](/images/jiqixuexi/jueceshu_jueceshu.png)

>决策树是根据训练数据来构造的，之后可用于分析数据，或用作预测。<br>
>优点：复杂度不高，便于理解。<br>
>缺点：可能会产生过度匹配问题。<br>
>难点：<br>
>   1 如果有很多特征，选取哪些特征，按照什么顺序才能更好的进行分类？<br>
    2 递归构造以及结束条件的判断。<br>
    3 过度匹配问题的处理。<br>
    4 通过训练集构造决策树，如何将其保存？<br>

>有兴趣可以先看看关于决策树的介绍：[决策树学习(wikipedia)](http://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0)

## 划分数据集
--------------------------------
>在学习如何选取最好的特征来划分数据之前，我们先来学习下<strong>熵</strong>(entropy)的概念。熵是集合信息的一种度量方式，可以表示划分数据集的前后信息发生的变化。它的公式为：<br>
>![熵](/images/jiqixuexi/jueceshu_shang.png)

>假设分类后，有C种结果，每种的概率为p(x)，结果切分的越均匀，entropy的值越大。所以，我们的目标是计算每一种特征的熵，选择最大值的特征，作为当前分类的判断节点。
>关于熵的介绍：[熵(wikipedia)](http://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5)

>另一种度量集合无序程度的方法是基尼不纯度，简单地说就是从一个数据集中随机选取子项，度量其被错误分类到其他分组的概率。基尼不纯度可以通过如下计算：
>![基尼不纯度](/images/jiqixuexi/jueceshu_jini.PNG)<br>

>然后。对每个特征划分一次数据集，计算数据集的新熵值，之后比较并返回最好特征划分的索引值。

## 递归构建决策树
--------------------------------
>划分一次后，数据将向下传递到树分支的下个节点，在这个节点上，我们可以再次划分数据，因此我们可以采用递归的原则处理数据集。但递归总是要有结束条件的，这里的结束条件是什么呢？<br>

>递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。若得到的关键字的值是类标签，则表示其是一个叶子节点，否则是一个判断节点，继续构造。<br>

>但是假如数据集已经处理了所有的属性，但是类标签依然不是唯一的。怎么办？我们通常会采用多数表决非方法决定该叶子节点的分类。<br>
>构造决策树的目的在于后续的使用，若是每次都根据训练集来生成，则会消耗很多时间。因此最好将生成好的决策树进行序列化操作，保存在硬盘上，并在需要的时候读取出来。<br>


