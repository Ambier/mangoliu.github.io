#机器学习_决策树
##介绍
--------------------------------
>举个简单的例子，先有个感性的认识：他人给介绍相亲时，总会问几个简单的问题，然后判断是否有见面的必要。这个过程实际上就是一个决策树。我们使用决策树来处理分类问题，它包含判断模块，终止模块。先看一个简单的决策树表示图：

<center>![决策树举例](/images/jiqixuexi/jueceshu_jueceshu.png)</center>

>决策树是根据训练数据来构造的，之后可用于分析数据，或用作预测。<br>
>优点：复杂度不高，便于理解。<br>
>缺点：可能会产生过度匹配问题。<br>
>难点：<br>
>   1 如果有很多特征，选取哪些特征，按照什么顺序才能更好的进行分类？<br>
    2 递归构造以及结束条件的判断。<br>
    3 过度匹配问题的处理。<br>
    4 通过训练集构造决策树，如何将其保存？<br>

>有兴趣可以先看看关于决策树的介绍：[决策树学习(wikipedia)](http://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0)

## 划分数据集
--------------------------------
>在学习如何选取最好的特征来划分数据之前，我们先来学习下熵(entropy)的概念。熵是集合信息的一种度量方式，可以表示划分数据集的前后信息发生的变化。它的公式为：<br>
>![熵](/images/jiqixuexi/jueceshu_shang.png)

>假设分类后，有C种结果，每种的概率为p(x)，结果切分的越均匀，entropy的值越大。所以，我们的目标是计算每一种特征的熵，选择最大值的特征，作为当前分类的判断节点。
>关于熵的介绍：[熵(wikipedia)](http://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5)

>另一种度量集合无序程度的方法是基尼不纯度，简单地说就是从一个数据集中随机选取子项，度量其被错误分类到其他分组的概率。基尼不纯度可以通过如下计算：
>![基尼不纯度](/images/jiqixuexi/jueceshu_jini.PNG)



